{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X-DX0ddMlMo3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import missingno as msno\n",
    "from scipy.stats import randint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error,r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "import ast\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mqPdlUrPwx3q"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>backdrop_path</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>...</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>keywords</th>\n",
       "      <th>cast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>/c7U9Fuy74WLp7gFAdpQJHn2T2no.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195000000</td>\n",
       "      <td>[{'id': 14, 'name': 'Fantasy'}, {'id': 28, 'na...</td>\n",
       "      <td>http://jackthegiantkiller.warnerbros.com</td>\n",
       "      <td>81005</td>\n",
       "      <td>tt1351685</td>\n",
       "      <td>en</td>\n",
       "      <td>Jack the Giant Slayer</td>\n",
       "      <td>...</td>\n",
       "      <td>114</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Prepare for a giant adventure</td>\n",
       "      <td>Jack the Giant Slayer</td>\n",
       "      <td>False</td>\n",
       "      <td>5.800</td>\n",
       "      <td>4690</td>\n",
       "      <td>['based on fairy tale', 'giant']</td>\n",
       "      <td>['Eleanor Tomlinson', 'Nicholas Hoult', 'Ewan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>/mV1HOCbUqx7nfFPwledYsvMYHrw.jpg</td>\n",
       "      <td>{'id': 1570, 'name': 'Die Hard Collection', 'p...</td>\n",
       "      <td>92000000</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n",
       "      <td>http://www.diehardmovie.com/</td>\n",
       "      <td>47964</td>\n",
       "      <td>tt1606378</td>\n",
       "      <td>en</td>\n",
       "      <td>A Good Day to Die Hard</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Yippee Ki-Yay Mother Russia</td>\n",
       "      <td>A Good Day to Die Hard</td>\n",
       "      <td>False</td>\n",
       "      <td>5.328</td>\n",
       "      <td>6099</td>\n",
       "      <td>['cia', 'escape', 'bomb', 'courthouse', 'agent...</td>\n",
       "      <td>['Bruce Willis', 'Jai Courtney', 'Sebastian Ko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>/eoIQMUeJDOG41way1pb5TjiPFFf.jpg</td>\n",
       "      <td>{'id': 111751, 'name': 'Texas Chainsaw Massacr...</td>\n",
       "      <td>10000000</td>\n",
       "      <td>[{'id': 27, 'name': 'Horror'}, {'id': 53, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76617</td>\n",
       "      <td>tt1572315</td>\n",
       "      <td>en</td>\n",
       "      <td>Texas Chainsaw 3D</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Evil wears many faces.</td>\n",
       "      <td>Texas Chainsaw 3D</td>\n",
       "      <td>False</td>\n",
       "      <td>5.470</td>\n",
       "      <td>1551</td>\n",
       "      <td>['home', 'gore', 'sequel', 'leatherface', 'sla...</td>\n",
       "      <td>['Alexandra Daddario', 'Dan Yeager', 'Trey Son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>/mJSe5dxKu8Sq0GfdjdWVqdGvzfV.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000000</td>\n",
       "      <td>[{'id': 14, 'name': 'Fantasy'}, {'id': 27, 'na...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60304</td>\n",
       "      <td>tt1428538</td>\n",
       "      <td>en</td>\n",
       "      <td>Hansel &amp; Gretel: Witch Hunters</td>\n",
       "      <td>...</td>\n",
       "      <td>88</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Classic tale. New twist.</td>\n",
       "      <td>Hansel &amp; Gretel: Witch Hunters</td>\n",
       "      <td>False</td>\n",
       "      <td>6.034</td>\n",
       "      <td>6367</td>\n",
       "      <td>['witch', 'gun', 'black magic', 'troll', 'stea...</td>\n",
       "      <td>['Jeremy Renner', 'Gemma Arterton', 'Famke Jan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>/mgnMFeLCEaNTOiD7w2DOaqzddyJ.jpg</td>\n",
       "      <td>{'id': 251937, 'name': 'A Haunted House Collec...</td>\n",
       "      <td>2500000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 27, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139038</td>\n",
       "      <td>tt2243537</td>\n",
       "      <td>en</td>\n",
       "      <td>A Haunted House</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>[{'english_name': 'English', 'iso_639_1': 'en'...</td>\n",
       "      <td>Released</td>\n",
       "      <td>This $*%! ain't paranormal.</td>\n",
       "      <td>A Haunted House</td>\n",
       "      <td>False</td>\n",
       "      <td>5.869</td>\n",
       "      <td>1732</td>\n",
       "      <td>['ghostbuster', 'haunted house', 'parody', 'cr...</td>\n",
       "      <td>['Marlon Wayans', 'Essence Atkins', 'Nick Swar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                     backdrop_path  \\\n",
       "0  False  /c7U9Fuy74WLp7gFAdpQJHn2T2no.jpg   \n",
       "1  False  /mV1HOCbUqx7nfFPwledYsvMYHrw.jpg   \n",
       "2  False  /eoIQMUeJDOG41way1pb5TjiPFFf.jpg   \n",
       "3  False  /mJSe5dxKu8Sq0GfdjdWVqdGvzfV.jpg   \n",
       "4  False  /mgnMFeLCEaNTOiD7w2DOaqzddyJ.jpg   \n",
       "\n",
       "                               belongs_to_collection     budget  \\\n",
       "0                                                NaN  195000000   \n",
       "1  {'id': 1570, 'name': 'Die Hard Collection', 'p...   92000000   \n",
       "2  {'id': 111751, 'name': 'Texas Chainsaw Massacr...   10000000   \n",
       "3                                                NaN   50000000   \n",
       "4  {'id': 251937, 'name': 'A Haunted House Collec...    2500000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 14, 'name': 'Fantasy'}, {'id': 28, 'na...   \n",
       "1  [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...   \n",
       "2  [{'id': 27, 'name': 'Horror'}, {'id': 53, 'nam...   \n",
       "3  [{'id': 14, 'name': 'Fantasy'}, {'id': 27, 'na...   \n",
       "4  [{'id': 35, 'name': 'Comedy'}, {'id': 27, 'nam...   \n",
       "\n",
       "                                   homepage      id    imdb_id  \\\n",
       "0  http://jackthegiantkiller.warnerbros.com   81005  tt1351685   \n",
       "1              http://www.diehardmovie.com/   47964  tt1606378   \n",
       "2                                       NaN   76617  tt1572315   \n",
       "3                                       NaN   60304  tt1428538   \n",
       "4                                       NaN  139038  tt2243537   \n",
       "\n",
       "  original_language                  original_title  ... runtime  \\\n",
       "0                en           Jack the Giant Slayer  ...     114   \n",
       "1                en          A Good Day to Die Hard  ...      98   \n",
       "2                en               Texas Chainsaw 3D  ...      92   \n",
       "3                en  Hansel & Gretel: Witch Hunters  ...      88   \n",
       "4                en                 A Haunted House  ...      86   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0  [{'english_name': 'English', 'iso_639_1': 'en'...  Released   \n",
       "1  [{'english_name': 'English', 'iso_639_1': 'en'...  Released   \n",
       "2  [{'english_name': 'English', 'iso_639_1': 'en'...  Released   \n",
       "3  [{'english_name': 'English', 'iso_639_1': 'en'...  Released   \n",
       "4  [{'english_name': 'English', 'iso_639_1': 'en'...  Released   \n",
       "\n",
       "                         tagline                           title  video  \\\n",
       "0  Prepare for a giant adventure           Jack the Giant Slayer  False   \n",
       "1    Yippee Ki-Yay Mother Russia          A Good Day to Die Hard  False   \n",
       "2         Evil wears many faces.               Texas Chainsaw 3D  False   \n",
       "3       Classic tale. New twist.  Hansel & Gretel: Witch Hunters  False   \n",
       "4    This $*%! ain't paranormal.                 A Haunted House  False   \n",
       "\n",
       "   vote_average  vote_count  \\\n",
       "0         5.800        4690   \n",
       "1         5.328        6099   \n",
       "2         5.470        1551   \n",
       "3         6.034        6367   \n",
       "4         5.869        1732   \n",
       "\n",
       "                                            keywords  \\\n",
       "0                   ['based on fairy tale', 'giant']   \n",
       "1  ['cia', 'escape', 'bomb', 'courthouse', 'agent...   \n",
       "2  ['home', 'gore', 'sequel', 'leatherface', 'sla...   \n",
       "3  ['witch', 'gun', 'black magic', 'troll', 'stea...   \n",
       "4  ['ghostbuster', 'haunted house', 'parody', 'cr...   \n",
       "\n",
       "                                                cast  \n",
       "0  ['Eleanor Tomlinson', 'Nicholas Hoult', 'Ewan ...  \n",
       "1  ['Bruce Willis', 'Jai Courtney', 'Sebastian Ko...  \n",
       "2  ['Alexandra Daddario', 'Dan Yeager', 'Trey Son...  \n",
       "3  ['Jeremy Renner', 'Gemma Arterton', 'Famke Jan...  \n",
       "4  ['Marlon Wayans', 'Essence Atkins', 'Nick Swar...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading all the CSV files\n",
    "file_paths = [\n",
    "    'Yearly\\movies2013.csv',\n",
    "    'Yearly\\movies2014.csv',\n",
    "    'Yearly\\movies2015.csv',\n",
    "    'Yearly\\movies2016.csv',\n",
    "    'Yearly\\movies2017.csv',\n",
    "    'Yearly\\movies2018.csv',\n",
    "    'Yearly\\movies2019.csv',\n",
    "    'Yearly\\movies2020.csv',\n",
    "    'Yearly\\movies2021.csv',\n",
    "    'Yearly\\movies2022.csv',\n",
    "    'Yearly\\movies2023.csv'\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "# Reading all CSV files into dataframes\n",
    "dataframes = [pd.read_csv(file) for file in file_paths]\n",
    "\n",
    "# Merging all dataframes into a single dataframe and dropping duplicates\n",
    "TMDB_df = pd.concat(dataframes)\n",
    "TMDB_df.drop_duplicates(subset=['imdb_id'], keep='first', inplace=True)\n",
    "TMDB_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOJO_df = pd.read_csv('Addit\\gross_earnings_domestic_2013-2023.csv').rename(columns={'daily earnings':'gross_earnings_domestic'})\n",
    "MOJO_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube=pd.read_csv('movie_engagement_data.csv')\n",
    "youtube.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomato=pd.read_csv('all_scores.csv')\n",
    "tomato.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read the CSV file from wikipedia into a DataFrame\n",
    "# wikipedia_budget = pd.read_csv('wikipedia_us_budget.csv', na_values=[\"\", \"#N/A\", \"[]\", \"0\"], encoding='latin-1')\n",
    "\n",
    "# #Convert the 'final_budget' column to a numeric type\n",
    "# wikipedia_budget['final_budget'] = pd.to_numeric(wikipedia_budget['final_budget'], errors='coerce')\n",
    "# wikipedia_budget.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General info. of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PfCOJ5a7Bl5T",
    "outputId": "4d95838a-66e4-4efd-ecb3-13fd1c8a38ba"
   },
   "outputs": [],
   "source": [
    "#filter in only rows with non-zero revenue\n",
    "TMDB_df = TMDB_df[TMDB_df['revenue']!=0]\n",
    "TMDB_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_df.loc[TMDB_df['title']==TMDB_df['original_title'],['title','original_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMDB_df.loc[TMDB_df['title']!=TMDB_df['original_title'],['title','original_title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows in TMDB df have rows where the movie title changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1eHOFI1B3eV",
    "outputId": "53abfc16-9db8-43b4-976e-2bdacdf85555"
   },
   "outputs": [],
   "source": [
    "#Check # of rows that are the same movie before merging\n",
    "MOJO_set = (set(MOJO_df[\"movie\"].to_list()))\n",
    "TMDB_set = set((TMDB_df[\"title\"].to_list()))\n",
    "print(f'Total movies in both dataframes: {len(MOJO_set.intersection(TMDB_set))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check # of rows that are the same movie before merging\n",
    "MOJO_set = (set(MOJO_df[\"movie\"].to_list()))\n",
    "TMDB_set = set((TMDB_df[\"original_title\"].to_list()))\n",
    "print(f'Total movies in both dataframes: {len(MOJO_set.intersection(TMDB_set))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39G81AxWCKP1",
    "outputId": "f8a8f839-d7d5-44e8-8483-b67d7c6d4ca7"
   },
   "outputs": [],
   "source": [
    "# Create sets of movie names from both DataFrames\n",
    "movie_names_df1 = set(MOJO_set)\n",
    "movie_names_df2 = set(TMDB_set)\n",
    "\n",
    "# Find movie names that are unique to each DataFrame\n",
    "unique_to_df1 = movie_names_df1 - movie_names_df2\n",
    "unique_to_df2 = movie_names_df2 - movie_names_df1\n",
    "\n",
    "# Print the movie names unique to each DataFrame\n",
    "print(\"Movies unique to MOJO_set:\", unique_to_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge on 'title' and assign dataframe to merged_title\n",
    "merged_title = pd.merge(MOJO_df,TMDB_df,left_on='movie',right_on='title',how='inner')\n",
    "\n",
    "#Merge on 'original_title' and assign dataframe to merged_original_title\n",
    "merged_original_title = pd.merge(MOJO_df,TMDB_df,left_on='movie',right_on='original_title',how='inner')\n",
    "\n",
    "#Concat the two dataframes by row and remove duplicates\n",
    "merged_df = pd.concat([merged_title,merged_original_title],axis=0)\n",
    "\n",
    "#Merge rotten tomatoes data\n",
    "merged_df = pd.merge(merged_df,tomato,left_on='movie',right_on='Movie',how='inner').drop(['Movie'],axis=1)\n",
    "\n",
    "#Merge youtube data\n",
    "merged_df = pd.merge(merged_df,youtube,left_on='movie',right_on='MovieTitle',how='inner').drop(['MovieTitle'],axis=1)\n",
    "\n",
    "#Drop duplicated rows\n",
    "merged_df.drop_duplicates(subset=['movie'], keep='first', inplace=True)\n",
    "merged_df = merged_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Now you have the 'wikipedia_budget' DataFrame with the data from the GitHub repository.\n",
    "# #Merge processed_df and wikipedia_budget on 'imdb_id'\n",
    "# merged_df = merged_df.merge(wikipedia_budget, on='imdb_id', how='left')\n",
    "\n",
    "# #Replace 'revenue' with 'final_budget' where 'revenue' is less than 10000\n",
    "# merged_df['budget'] = merged_df.apply(lambda row: row['final_budget'] if (row['revenue']<10000) else row['budget'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(x):\n",
    "    if x is np.nan:\n",
    "        return x\n",
    "    else:\n",
    "        x = x.replace(\"'\",'\"').replace('[','').replace(']','')\n",
    "        name_start = x.find('\"name\":')\n",
    "        poster_path_start = x.find('\"poster_path\":')\n",
    "        \n",
    "        if '\"' in x[name_start+9:poster_path_start-3]:\n",
    "            parsed_x = x[:name_start+9] + x[name_start+9:poster_path_start-3].replace('\"',\"'\") + x[poster_path_start-3:]\n",
    "            \n",
    "            if 'None' in x:\n",
    "                return json.loads(parsed_x.replace('None','null'))['name']\n",
    "            else:\n",
    "                return json.loads(parsed_x)['name']            \n",
    "        else:\n",
    "            if 'None' in x:\n",
    "                return json.loads(x.replace('None','null'))['name']\n",
    "            else:\n",
    "                return json.loads(x)['name']\n",
    "\n",
    "def get_genres(x):\n",
    "    if x is np.nan:\n",
    "        return x\n",
    "    else:\n",
    "        if '[' in x:\n",
    "            genres_list = []\n",
    "            x= x.replace(\"'\",'\"')\n",
    "            for i in range(len(json.loads(x))):\n",
    "                genres_list.append(json.loads(x)[i]['name'])\n",
    "            return genres_list\n",
    "        else:\n",
    "            return [x]\n",
    "        \n",
    "def get_production_companies(x):\n",
    "    if (x == '[]')|(x is np.nan):\n",
    "        return np.nan\n",
    "    elif '[' in x:\n",
    "        prod_comp_list = []\n",
    "        for data_list in ast.literal_eval(x):\n",
    "            prod_comp_list.append(data_list['name'])\n",
    "        return prod_comp_list\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "def get_spoken_language(x):\n",
    "    if (x == '[]')|(x is np.nan):\n",
    "        return np.nan\n",
    "    elif '[' in x:\n",
    "        prod_comp_list = []\n",
    "        for data_list in ast.literal_eval(x):\n",
    "            prod_comp_list.append(data_list['iso_639_1'])\n",
    "        return prod_comp_list\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "def get_keywords(x):\n",
    "    if (x == '[]')|(x is np.nan):\n",
    "        return np.nan\n",
    "    elif '[' in x:\n",
    "        return ast.literal_eval(x)\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "def get_cast(x):\n",
    "    if (x == '[]')|(x is np.nan):\n",
    "        return np.nan\n",
    "    elif '[' in x:\n",
    "        return ast.literal_eval(x)\n",
    "    else:\n",
    "        return [x]\n",
    "\n",
    "def convert_date_string_to_datetime(date_string):\n",
    "    try:\n",
    "        return pd.to_datetime(date_string, errors='coerce')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    \"\"\"\n",
    "    convert json columns into lists and return dataframe with processed data\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    #Drop irrelevant columns:\n",
    "    #Columns 'adult', 'status', and 'video' are dropped because they all have the same values across all rows\n",
    "    processed_df = processed_df.drop(['adult','backdrop_path','homepage','id','imdb_id','original_title',\n",
    "                                      'poster_path','status','title','video','YouTube Video Title','YouTube Video ID'],axis=1)\n",
    "    \n",
    "    #Fix columns that are in json format\n",
    "    processed_df['belongs_to_collection'] = processed_df['belongs_to_collection'].apply(get_collection)\n",
    "    processed_df['genres'] = processed_df['genres'].apply(get_genres)\n",
    "    processed_df['production_companies'] = processed_df['production_companies'].apply(get_production_companies)\n",
    "    processed_df['production_countries'] = processed_df['production_countries'].apply(get_production_companies)\n",
    "    processed_df['release_date'] = processed_df['release_date'].apply(convert_date_string_to_datetime)\n",
    "    processed_df['spoken_languages'] = processed_df['spoken_languages'].apply(get_spoken_language)\n",
    "    processed_df['keywords'] = processed_df['keywords'].apply(get_keywords)\n",
    "    processed_df['cast'] = processed_df['cast'].apply(get_cast)\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Budget has a lot of 0 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = process_data(merged_df)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('final_data_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing + Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"C:\\Users\\ASUS\\Documents\\UofT MEng\\Fall 2023-2024\\CME538\\Big project\\GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform word embedding on a given column\n",
    "def word_embed(x,model):\n",
    "    overview_vec = nltk.word_tokenize(x.lower())\n",
    "    vectors = [model.get_vector(word) for word in overview_vec if word in model.key_to_index]\n",
    "    vector_final = np.sum(vectors,axis=0)/len(vectors)\n",
    "    return vector_final.reshape(1,-1)[0]\n",
    "\n",
    "# Function to assign each element in the array to new columns\n",
    "def assign_embedding_columns(row):\n",
    "    embeddings = row['overview_embedded'].flatten()  # Flatten the array to a 1D array\n",
    "    columns = [f'overview_embedding_{i+1}' for i in range(embeddings.shape[0])]\n",
    "    return pd.Series(embeddings, index=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to featurize training and testing sets\n",
    "def featurize(df):\n",
    "    \"\"\"\n",
    "    input: dataset containing target variable and features\n",
    "    output: dataset with features and target ready to be used in model\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df = df.set_index('movie')\n",
    "    featurized_df = pd.DataFrame()\n",
    "    \n",
    "    #Separate numerical features\n",
    "    num_features = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "    \n",
    "    df = df.dropna(subset=['production_companies','production_countries','cast','spoken_languages'])\n",
    "    \n",
    "    #Create more features from release date\n",
    "    df['year'] = df['release_date'].dt.year\n",
    "    df['month'] = df['release_date'].dt.month\n",
    "    season_mapping = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}\n",
    "    df['season'] = (df['release_date'].dt.month % 12 + 3) // 3\n",
    "    df['season'] = df['season'].map(season_mapping)\n",
    "\n",
    "    #Encode cat features\n",
    "    #Using Binarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    genres_encoded = pd.DataFrame(mlb.fit_transform(df['genres']), columns=[f'genres_{x}' for x in mlb.classes_], index=df.index)\n",
    "    production_countries_encoded = pd.DataFrame(mlb.fit_transform(df['production_countries']), columns=[f'production_{x}' for x in mlb.classes_], index=df.index)\n",
    "    spoken_language_encoded = pd.DataFrame(mlb.fit_transform(df['spoken_languages']), columns=[f'spoken_language_{x}' for x in mlb.classes_], index=df.index)\n",
    "    \n",
    "    #Use get_dummies\n",
    "    #original_language_encoded = df['original_language'].str.get_dummies(prefix='original_language', prefix_sep='_')\n",
    "    original_language_encoded = pd.get_dummies(df['original_language'],prefix='original_language', prefix_sep='_')\n",
    "    year_encoded = pd.get_dummies(df['year'],prefix='Year', prefix_sep='_')\n",
    "    month_encoded = pd.get_dummies(df['month'],prefix='month', prefix_sep='_')\n",
    "    season_encoded = pd.get_dummies(df['season'],prefix='season',prefix_sep='_')\n",
    "    \n",
    "    \n",
    "    #Word embedding\n",
    "    df['overview_embedded'] = df['overview'].apply(lambda x: word_embed(x,word2vec_model))\n",
    "    overview_embeddings = df.apply(assign_embedding_columns, axis=1)\n",
    "\n",
    "    featurized_df = pd.concat([genres_encoded,production_countries_encoded,spoken_language_encoded,\n",
    "                               original_language_encoded,overview_embeddings,year_encoded,month_encoded,\n",
    "                               season_encoded,df.loc[:,num_features]],axis=1)\n",
    "    \n",
    "    featurized_df = featurized_df.reindex(df.index)\n",
    "    return featurized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get potential features from processed dataset\n",
    "featurized_df = featurize(processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correlation(df,feature,threshold):\n",
    "    columns = [col for col in df.columns if feature in col]\n",
    "    columns.append('revenue')\n",
    "    correlation_matrix = df.loc[:,columns].corr(method= 'pearson')\n",
    "    \n",
    "    filtered_column_list = []\n",
    "    for key, value in correlation_matrix.loc['revenue',:].items():\n",
    "        if abs(value)>threshold:\n",
    "            filtered_column_list.append(key)\n",
    "        else:\n",
    "            continue\n",
    "    filtered_column_list.remove('revenue')\n",
    "    return filtered_column_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define genres columns\n",
    "genres_columns = [col for col in featurized_df.columns if 'genre' in col]\n",
    "genres_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,genres_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'genres' based on correlation threshold of 0.1\n",
    "filter_correlation(featurized_df,'genre',0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define poduction countries columns\n",
    "prod_countries_columns = [col for col in featurized_df.columns if 'production_' in col]\n",
    "prod_countries_columns = prod_countries_columns[:18]\n",
    "prod_countries_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,prod_countries_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define poduction countries columns\n",
    "prod_countries_columns = [col for col in featurized_df.columns if 'production_' in col]\n",
    "prod_countries_columns = prod_countries_columns[18:36]\n",
    "prod_countries_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,prod_countries_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define poduction countries columns\n",
    "prod_countries_columns = [col for col in featurized_df.columns if 'production_' in col]\n",
    "prod_countries_columns = prod_countries_columns[36:54]\n",
    "prod_countries_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,prod_countries_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define poduction countries columns\n",
    "prod_countries_columns = [col for col in featurized_df.columns if 'production_' in col]\n",
    "prod_countries_columns = prod_countries_columns[54:]\n",
    "prod_countries_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,prod_countries_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'production_countries' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'production_',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spoken language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "spoken_lang_columns = [col for col in featurized_df.columns if 'spoken_' in col]\n",
    "spoken_lang_columns = spoken_lang_columns[:18]\n",
    "spoken_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,spoken_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "spoken_lang_columns = [col for col in featurized_df.columns if 'spoken_' in col]\n",
    "spoken_lang_columns = spoken_lang_columns[18:36]\n",
    "spoken_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,spoken_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "spoken_lang_columns = [col for col in featurized_df.columns if 'spoken_' in col]\n",
    "spoken_lang_columns = spoken_lang_columns[36:54]\n",
    "spoken_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,spoken_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "spoken_lang_columns = [col for col in featurized_df.columns if 'spoken_' in col]\n",
    "spoken_lang_columns = spoken_lang_columns[54:]\n",
    "spoken_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,spoken_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'spoken_languages' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'spoken_',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "original_lang_columns = [col for col in featurized_df.columns if 'original' in col]\n",
    "original_lang_columns = original_lang_columns[:15]\n",
    "original_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,original_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "original_lang_columns = [col for col in featurized_df.columns if 'original' in col]\n",
    "original_lang_columns = original_lang_columns[15:]\n",
    "original_lang_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,original_lang_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'original_languages' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'original',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define spoken language columns\n",
    "year_columns = [col for col in featurized_df.columns if 'Year' in col]\n",
    "year_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,year_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'Year' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'Year',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define month columns\n",
    "month_columns = [col for col in featurized_df.columns if 'month' in col]\n",
    "month_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation_matrix = featurized_df.loc[:,month_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'month' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'month_',0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define month columns\n",
    "season_columns = [col for col in featurized_df.columns if 'season' in col]\n",
    "season_columns.append('revenue')\n",
    "\n",
    "# Assuming df_encoded is the DataFrame containing one-hot encoded features\n",
    "plt.figure(figsize=(10,10))\n",
    "correlation_matrix = featurized_df.loc[:,season_columns].corr(method= 'pearson')\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Point-Biserial Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter encoded features from 'season' based on correlation threshold of 0.05\n",
    "filter_correlation(featurized_df,'season_',0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Try PCA on word embeddings\n",
    "embedding_columns = [x for x in featurized_df.columns if 'embed' in x]\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(featurized_df.loc[:,embedding_columns])\n",
    "\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Filter the rows in processed_df based on the conditions\n",
    "processed_df = processed_df[~((processed_df['revenue']<10000)|(processed_df['revenue']<processed_df['gross_earnings_domestic']))]\n",
    "processed_df = processed_df.drop(columns=['gross_earnings_domestic','earnings_per_theatre','vote_count','vote_average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 0 in budget with nan\n",
    "processed_df['budget'] = processed_df['budget'].replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "train,test = train_test_split(processed_df,test_size=0.3,random_state=0)\n",
    "\n",
    "#Featurize training and testing features\n",
    "train = featurize(train)\n",
    "test = featurize(test)\n",
    "\n",
    "#Separate target and features from dataset\n",
    "y_train = train[['revenue']]\n",
    "y_test = test[['revenue']]\n",
    "X_train = train.drop(columns=['revenue'])\n",
    "X_test = test.drop(columns=['revenue'])\n",
    "\n",
    "#Initialize empty dataframe to store final training and testing features\n",
    "X_train_final = pd.DataFrame()\n",
    "X_test_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA on word embeddings\n",
    "embedding_columns = [x for x in X_train.columns if 'embed' in x]\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_train.loc[:,embedding_columns])\n",
    "\n",
    "#Define columns for new transformed word embedding features\n",
    "pca_columns = [f'word_embed_PC{i+1}' for i in range(pca.transform(X_train.loc[:,embedding_columns]).shape[1])]\n",
    "\n",
    "train_pca = pd.DataFrame(data=pca.transform(X_train.loc[:,embedding_columns]),columns=pca_columns,index=X_train.index)\n",
    "test_pca = pd.DataFrame(data=pca.transform(X_test.loc[:,embedding_columns]),columns=pca_columns,index=X_test.index)\n",
    "\n",
    "#Concat tranformed features to final features\n",
    "X_train_final = pd.concat([X_train_final,train_pca],axis=1)\n",
    "X_test_final = pd.concat([X_test_final,test_pca],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define list of numerical features\n",
    "num_features = processed_df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "num_features = list(set(num_features).difference(y_train.columns.tolist()))\n",
    "\n",
    "\n",
    "#KNNImpute nan values in column 'budget'\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputer.fit(X_train.loc[:,num_features])\n",
    "X_train[num_features] = imputer.transform(X_train.loc[:,num_features])\n",
    "X_test[num_features] = imputer.transform(X_test.loc[:,num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.loc[:,num_features].copy())\n",
    "\n",
    "X_train_num_scaled = pd.DataFrame(data=scaler.transform(X_train.loc[:,num_features]),columns=num_features,index=X_train.index)\n",
    "X_test_num_scaled = pd.DataFrame(data=scaler.transform(X_test.loc[:,num_features]),columns=num_features,index=X_test.index)\n",
    "\n",
    "#Add numerical features to final features\n",
    "X_train_final = pd.concat([X_train_final,X_train_num_scaled],axis=1)\n",
    "X_test_final = pd.concat([X_test_final,X_test_num_scaled],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get encoded categorical features that will be selected before RFECV\n",
    "cat_columns_train = filter_correlation(train,'genres',0.1)\n",
    "\n",
    "for i in ['production_','spoken_','original','Year','month','season']:\n",
    "    cat_columns_train += filter_correlation(train,i,0.05)\n",
    "\n",
    "cat_columns_final = list(set(cat_columns_train).intersection(X_test.columns))\n",
    "\n",
    "X_train_final = pd.concat([X_train_final,X_train.loc[:,cat_columns_final]],axis=1)\n",
    "X_test_final = pd.concat([X_test_final,X_test.loc[:,cat_columns_final]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = RandomForestRegressor()\n",
    "# selector = RFECV(estimator, step=2, cv=5)\n",
    "# selector = selector.fit(X_train_final, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train_final,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = r2_score(y_test.values.ravel(), rfr.predict(X_test_final))\n",
    "MSE = mean_squared_error(y_test.values.ravel(), rfr.predict(X_test_final))\n",
    "print(f'MSE: {MSE}\\nR2 score: {R2}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
